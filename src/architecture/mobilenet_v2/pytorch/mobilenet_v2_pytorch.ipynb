{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9qCB-WM0t1m"
   },
   "source": [
    "# I) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This paper introduces the MobileNet-V2 architecture which is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers.\n",
    "- Authors showed it is important to remove non-linearities in the narrow layers in order to maintain representational power and to improve performance.\n",
    "- It results in a very memory-efficient inference model.\n",
    "\n",
    "## 1) Depthwise separable convolution\n",
    "\n",
    "- To understand what a depthwise separable convolution really is, let's compare it to a normal convolution between a 12x12x3 input and 256 kernels of size 5x5x3.\n",
    "- A depthwise separable convolution is divided into 2 parts:\n",
    "    - **Depthwise convolution**.\n",
    "    - **Pointwise convolution**.\n",
    " \n",
    "**<ins>Depthwise convolution: </ins>**\n",
    "\n",
    "- In a normal convolution, **all channels** of a kernel are used to produce a feature map.\n",
    "- In a depthwise convolution, **each channel** of a kernel is used to produce a feature map.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/1.png\" width=\"60%\">\n",
    "    <figcaption > Figure: Normal convolution</figcaption>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/2.png\" width=\"60%\">\n",
    "    <figcaption > Figure: Depthwise convolution</figcaption>\n",
    "</div>\n",
    "\n",
    "**<ins>Pointwise convolution: </ins>**\n",
    "\n",
    "- To increase the number of channels in our output image to 256:\n",
    "    - In a **normal convolution**, we just have to use **256 filters of size 5x5x3**.\n",
    "    - In a **pointwise convolution**, we just have to use **256 filters of size 1x1x3**.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/3.png\" width=\"60%\">\n",
    "    <figcaption > Figure: Normal convolution</figcaption>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/4.png\" width=\"60%\">\n",
    "    <figcaption > Figure: Pointwise convolution</figcaption>\n",
    "</div>\n",
    "\n",
    "- **What's the main difference between a depthwise separable convolution and normal convolution ?**\n",
    "\n",
    "- The **main difference** is the **number of computations**. In our example:\n",
    "    - For a **normal convolution**, we have ((8x8x5x5)x3)x256 = **1,228,800** operations.\n",
    "    - For a **depthwise separable convolution**, we have 4800 + 49,152 = **53,952** operations:\n",
    "        - in a **depthwise convolution**, (8x8x5x5)x3 = 4800 operations.\n",
    "        - in a **pointwise convolution**, ((8x8x1x1)x3)x256 = 49,152 operations.\n",
    "        \n",
    "- We can clearly see that a depthwise separable convolution is **less expensive** than a normal convolution (~22.7% less computations).\n",
    "- The reason is, in a normal convolution, we are **transforming the image 256 times** whereas in a depthwise separable convolution, we transform the image **once** and then **expand it 256 times** along the channel axis.\n",
    "\n",
    "\n",
    "## 2) Linear bottleneck\n",
    "- Linear bottleneck layers are just bottleneck layers with a linear activation.\n",
    "- It was long assume that manifold of interest could be embedded in a low-dimensional subspace. In other words, the data representation that we are interested in could be embedded in a tiny subspace. \n",
    "- Thus, we can simply reduce the dimensionality of a layer until we get the manifold of interest.\n",
    "- However, this intuition breaks because deep neural network uses ReLU which squashes aways too much information if the features are already in low dimension (Fig. 1 illustrated this perfectly)\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/5.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "- But there are 2 properties that are indicative of the requirement that the manifold of interest should lie in a low-dimensional subspace (of the higher-dimensional activation space):\n",
    "    1. If the manifold of interest remains non-zero volume after ReLU transformation, it corresponds to a linear transformation.\n",
    "    2. ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space.\n",
    "<br>\n",
    "- Thus, if we assume the manifold of interest is low-dimensional, **we can capture this by inserting linear bottleneck layers into the convolutional blocks**.\n",
    "- Experimental evidences show that using non-linear layers in bottleneck destroy too much information in low-dimensional space even if in general, linear bottleneck models are strictly less powerful than models with non-linearities.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/6.png\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "## 3) Inverted Residuals\n",
    "- Original residual block contains an input followed by several bottlenecks then followed by expansion and the shortcuts exist between thick layers (layers with many channels).\n",
    "-  However, inspired by the intuition that the bottlenecks actually contain all the necessary information and expansion layer acts merely as a non-linear transformation, MobileNetV2 uses shortcuts directly between the bottlenecks (thin layers). Hatched layers use linear activation.\n",
    "- ReLU6 is used as the non-liner activation because of its robustnes when used with low-precision computation.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/7.png\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "- The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/8.png\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "## 4) Information flow interpretation\n",
    "\n",
    "- MobileNetV2 provides a natural separation between two things, which have been tangled together in previous architectures.\n",
    "    - Capacity: input/output domains of the building blocks (bottleneck layers).\n",
    "    - Expressiveness: layer transformation, a non-linear function that converts input to the output (expansion layers).\n",
    "     \n",
    "- Authors say that exploring these concepts separately is an important direction for future research.\n",
    "\n",
    "## 5) Architecture\n",
    "\n",
    "- This is a Fully Convolutional Network (no linear layers). Here is its architecture:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/9.png\" width=\"40%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/10.png\" width=\"40%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/11.png\" width=\"60%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/12.png\" width=\"80%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/valoxe/image-storage-1/master/research-paper-summary/mobilenet-v2/13.png\" width=\"80%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlZThWBY0t18"
   },
   "source": [
    "# II) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper doesn't explain how to deal with input/output channels mismatch during shortcut connections. Thus, shortcut connections will only be used when input/output channel match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6yYsFGI0t2D"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0YIJ-h20zL_"
   },
   "source": [
    "## a) Loading dataset / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cTfsQH50xm9"
   },
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    transform = transforms.Compose([transforms.Resize((96,96)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "            \n",
    "    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    #Clear downloading message.\n",
    "    clear_output()\n",
    "    \n",
    "    # Split dataset into training set and validation set.\n",
    "    train_dataset, val_dataset = random_split(train_dataset, (45000, 5000))\n",
    "    \n",
    "    print(\"Image Shape: {}\".format(train_dataset[0][0].numpy().shape), end = '\\n\\n')\n",
    "    print(\"Training Set:   {} samples\".format(len(train_dataset)))\n",
    "    print(\"Validation Set:   {} samples\".format(len(val_dataset)))\n",
    "    print(\"Test Set:       {} samples\".format(len(test_dataset)))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        BATCH_SIZE = 256\n",
    "    else:\n",
    "        BATCH_SIZE = 32\n",
    "\n",
    "    # Create iterator.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Delete the data/ folder.\n",
    "    shutil.rmtree('./data')\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109,
     "referenced_widgets": [
      "60689c3a4e9e450a9403d7aa47a759f3",
      "a8f5782fae1e4838877befe6c57350f5",
      "456e14f6725f42d183ffd82472813baf",
      "f00311b48f76429eb02379de9c257286",
      "0ea8e31611fc499995dedc0b3405f9be",
      "956d35386dc2413eaf35645f82c1b5bd",
      "c571fbb91dc84540903ab412cd8279bb",
      "f823f8422b7a490c883d00f0258fa11d"
     ]
    },
    "colab_type": "code",
    "id": "X4bCicVG1uow",
    "outputId": "df367ec9-7a74-46b8-883c-bcbef5bdd5da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (3, 96, 96)\n",
      "\n",
      "Training Set:   45000 samples\n",
      "Validation Set:   5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCEVHzDp025A"
   },
   "source": [
    "## b) Architecture build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT_o5UrD0t2O"
   },
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t, stride):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('pconv1', nn.Conv2d(in_channels,\n",
    "                                in_channels*t,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=0,\n",
    "                                bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(in_channels*t)),\n",
    "            ('act1', nn.ReLU6()),\n",
    "            ('dconv', nn.Conv2d(in_channels*t,\n",
    "                                in_channels*t,\n",
    "                                kernel_size=3,\n",
    "                                groups=in_channels*t,\n",
    "                                stride=stride,\n",
    "                                padding=1,\n",
    "                                bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(in_channels*t)),\n",
    "            ('act2', nn.ReLU6()),\n",
    "            ('pconv3', nn.Conv2d(in_channels*t,\n",
    "                                out_channels,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=0,\n",
    "                                bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(out_channels))\n",
    "        ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            out += x \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTJLPKsr0t2m"
   },
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, block_type, bottleneck_settings, width_multiplier, num_classes):\n",
    "        super(MobileNet, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.b_s = bottleneck_settings\n",
    "        self.b_s['c'] = [int(elt * width_multiplier) for elt in self.b_s['c']]\n",
    "        self.in_channels = int(32 * width_multiplier)\n",
    "        self.out_channels = int(1280 * width_multiplier)\n",
    "        \n",
    "        # Feature\n",
    "        self.conv0 = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, self.in_channels, 1, stride=2, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(self.in_channels)),\n",
    "            ('act0', nn.ReLU6()) \n",
    "        ]))\n",
    "        self.bottleneck1 = self.__build_layer(block_type,\n",
    "                                              self.in_channels, \n",
    "                                              self.b_s['c'][0], \n",
    "                                              self.b_s['t'][0],\n",
    "                                              self.b_s['s'][0],\n",
    "                                              self.b_s['n'][0])\n",
    "        self.bottleneck2 = self.__build_layer(block_type, \n",
    "                                              self.b_s['c'][0],\n",
    "                                              self.b_s['c'][1], \n",
    "                                              self.b_s['t'][1],\n",
    "                                              self.b_s['s'][1],\n",
    "                                              self.b_s['n'][1])\n",
    "        self.bottleneck3 = self.__build_layer(block_type,\n",
    "                                              self.b_s['c'][1],\n",
    "                                              self.b_s['c'][2],\n",
    "                                              self.b_s['t'][2],\n",
    "                                              self.b_s['s'][2],\n",
    "                                              self.b_s['n'][2])\n",
    "        self.bottleneck4 = self.__build_layer(block_type,\n",
    "                                              self.b_s['c'][2],\n",
    "                                              self.b_s['c'][3],\n",
    "                                              self.b_s['t'][3],\n",
    "                                              self.b_s['s'][3],\n",
    "                                              self.b_s['n'][3])\n",
    "        self.bottleneck5 = self.__build_layer(block_type,\n",
    "                                              self.b_s['c'][3],\n",
    "                                              self.b_s['c'][4],\n",
    "                                              self.b_s['t'][4],\n",
    "                                              self.b_s['s'][4],\n",
    "                                              self.b_s['n'][4])\n",
    "        self.bottleneck6 = self.__build_layer(block_type,\n",
    "                                              self.b_s['c'][4],\n",
    "                                              self.b_s['c'][5],\n",
    "                                              self.b_s['t'][5],\n",
    "                                              self.b_s['s'][5],\n",
    "                                              self.b_s['n'][5])\n",
    "        self.bottleneck7 = self.__build_layer(block_type,\n",
    "                                              self.b_s['c'][5],\n",
    "                                              self.b_s['c'][6],\n",
    "                                              self.b_s['t'][6],\n",
    "                                              self.b_s['s'][6],\n",
    "                                              self.b_s['n'][6])\n",
    "        # Classifier\n",
    "        self.conv8 = nn.Sequential(OrderedDict([\n",
    "            ('conv8', nn.Conv2d(self.b_s['c'][6], self.out_channels, 1, bias=False)),\n",
    "            ('bn8', nn.BatchNorm2d(self.out_channels)),\n",
    "            ('act8', nn.ReLU6()) \n",
    "        ]))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.conv9 = nn.Conv2d(self.out_channels, num_classes, 1)\n",
    "    \n",
    "    def __build_layer(self, block_type, in_channels, out_channels, t, s, n):\n",
    "        layers = []\n",
    "        tmp_channels = in_channels\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                layers.append(block_type(tmp_channels, out_channels, t, s))\n",
    "            else:\n",
    "                layers.append(block_type(tmp_channels, out_channels, t, 1))\n",
    "            tmp_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.bottleneck1(out)\n",
    "        out = self.bottleneck2(out)\n",
    "        out = self.bottleneck3(out)\n",
    "        out = self.bottleneck4(out)\n",
    "        out = self.bottleneck5(out)\n",
    "        out = self.bottleneck6(out)\n",
    "        out = self.bottleneck7(out)\n",
    "        out = self.conv8(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = self.conv9(out)\n",
    "        out = out.view(-1, self.num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "didqQWok0t2y",
    "outputId": "4aa199bf-62c4-4e59-aefa-2db8064a88c5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 48, 48]              48\n",
      "       BatchNorm2d-2           [-1, 16, 48, 48]              32\n",
      "             ReLU6-3           [-1, 16, 48, 48]               0\n",
      "            Conv2d-4           [-1, 16, 48, 48]             256\n",
      "       BatchNorm2d-5           [-1, 16, 48, 48]              32\n",
      "             ReLU6-6           [-1, 16, 48, 48]               0\n",
      "            Conv2d-7           [-1, 16, 48, 48]             144\n",
      "       BatchNorm2d-8           [-1, 16, 48, 48]              32\n",
      "             ReLU6-9           [-1, 16, 48, 48]               0\n",
      "           Conv2d-10            [-1, 8, 48, 48]             128\n",
      "      BatchNorm2d-11            [-1, 8, 48, 48]              16\n",
      "       Bottleneck-12            [-1, 8, 48, 48]               0\n",
      "           Conv2d-13           [-1, 48, 48, 48]             384\n",
      "      BatchNorm2d-14           [-1, 48, 48, 48]              96\n",
      "            ReLU6-15           [-1, 48, 48, 48]               0\n",
      "           Conv2d-16           [-1, 48, 24, 24]             432\n",
      "      BatchNorm2d-17           [-1, 48, 24, 24]              96\n",
      "            ReLU6-18           [-1, 48, 24, 24]               0\n",
      "           Conv2d-19           [-1, 12, 24, 24]             576\n",
      "      BatchNorm2d-20           [-1, 12, 24, 24]              24\n",
      "       Bottleneck-21           [-1, 12, 24, 24]               0\n",
      "           Conv2d-22           [-1, 72, 24, 24]             864\n",
      "      BatchNorm2d-23           [-1, 72, 24, 24]             144\n",
      "            ReLU6-24           [-1, 72, 24, 24]               0\n",
      "           Conv2d-25           [-1, 72, 24, 24]             648\n",
      "      BatchNorm2d-26           [-1, 72, 24, 24]             144\n",
      "            ReLU6-27           [-1, 72, 24, 24]               0\n",
      "           Conv2d-28           [-1, 12, 24, 24]             864\n",
      "      BatchNorm2d-29           [-1, 12, 24, 24]              24\n",
      "       Bottleneck-30           [-1, 12, 24, 24]               0\n",
      "           Conv2d-31           [-1, 72, 24, 24]             864\n",
      "      BatchNorm2d-32           [-1, 72, 24, 24]             144\n",
      "            ReLU6-33           [-1, 72, 24, 24]               0\n",
      "           Conv2d-34           [-1, 72, 12, 12]             648\n",
      "      BatchNorm2d-35           [-1, 72, 12, 12]             144\n",
      "            ReLU6-36           [-1, 72, 12, 12]               0\n",
      "           Conv2d-37           [-1, 16, 12, 12]           1,152\n",
      "      BatchNorm2d-38           [-1, 16, 12, 12]              32\n",
      "       Bottleneck-39           [-1, 16, 12, 12]               0\n",
      "           Conv2d-40           [-1, 96, 12, 12]           1,536\n",
      "      BatchNorm2d-41           [-1, 96, 12, 12]             192\n",
      "            ReLU6-42           [-1, 96, 12, 12]               0\n",
      "           Conv2d-43           [-1, 96, 12, 12]             864\n",
      "      BatchNorm2d-44           [-1, 96, 12, 12]             192\n",
      "            ReLU6-45           [-1, 96, 12, 12]               0\n",
      "           Conv2d-46           [-1, 16, 12, 12]           1,536\n",
      "      BatchNorm2d-47           [-1, 16, 12, 12]              32\n",
      "       Bottleneck-48           [-1, 16, 12, 12]               0\n",
      "           Conv2d-49           [-1, 96, 12, 12]           1,536\n",
      "      BatchNorm2d-50           [-1, 96, 12, 12]             192\n",
      "            ReLU6-51           [-1, 96, 12, 12]               0\n",
      "           Conv2d-52           [-1, 96, 12, 12]             864\n",
      "      BatchNorm2d-53           [-1, 96, 12, 12]             192\n",
      "            ReLU6-54           [-1, 96, 12, 12]               0\n",
      "           Conv2d-55           [-1, 16, 12, 12]           1,536\n",
      "      BatchNorm2d-56           [-1, 16, 12, 12]              32\n",
      "       Bottleneck-57           [-1, 16, 12, 12]               0\n",
      "           Conv2d-58           [-1, 96, 12, 12]           1,536\n",
      "      BatchNorm2d-59           [-1, 96, 12, 12]             192\n",
      "            ReLU6-60           [-1, 96, 12, 12]               0\n",
      "           Conv2d-61             [-1, 96, 6, 6]             864\n",
      "      BatchNorm2d-62             [-1, 96, 6, 6]             192\n",
      "            ReLU6-63             [-1, 96, 6, 6]               0\n",
      "           Conv2d-64             [-1, 32, 6, 6]           3,072\n",
      "      BatchNorm2d-65             [-1, 32, 6, 6]              64\n",
      "       Bottleneck-66             [-1, 32, 6, 6]               0\n",
      "           Conv2d-67            [-1, 192, 6, 6]           6,144\n",
      "      BatchNorm2d-68            [-1, 192, 6, 6]             384\n",
      "            ReLU6-69            [-1, 192, 6, 6]               0\n",
      "           Conv2d-70            [-1, 192, 6, 6]           1,728\n",
      "      BatchNorm2d-71            [-1, 192, 6, 6]             384\n",
      "            ReLU6-72            [-1, 192, 6, 6]               0\n",
      "           Conv2d-73             [-1, 32, 6, 6]           6,144\n",
      "      BatchNorm2d-74             [-1, 32, 6, 6]              64\n",
      "       Bottleneck-75             [-1, 32, 6, 6]               0\n",
      "           Conv2d-76            [-1, 192, 6, 6]           6,144\n",
      "      BatchNorm2d-77            [-1, 192, 6, 6]             384\n",
      "            ReLU6-78            [-1, 192, 6, 6]               0\n",
      "           Conv2d-79            [-1, 192, 6, 6]           1,728\n",
      "      BatchNorm2d-80            [-1, 192, 6, 6]             384\n",
      "            ReLU6-81            [-1, 192, 6, 6]               0\n",
      "           Conv2d-82             [-1, 32, 6, 6]           6,144\n",
      "      BatchNorm2d-83             [-1, 32, 6, 6]              64\n",
      "       Bottleneck-84             [-1, 32, 6, 6]               0\n",
      "           Conv2d-85            [-1, 192, 6, 6]           6,144\n",
      "      BatchNorm2d-86            [-1, 192, 6, 6]             384\n",
      "            ReLU6-87            [-1, 192, 6, 6]               0\n",
      "           Conv2d-88            [-1, 192, 6, 6]           1,728\n",
      "      BatchNorm2d-89            [-1, 192, 6, 6]             384\n",
      "            ReLU6-90            [-1, 192, 6, 6]               0\n",
      "           Conv2d-91             [-1, 32, 6, 6]           6,144\n",
      "      BatchNorm2d-92             [-1, 32, 6, 6]              64\n",
      "       Bottleneck-93             [-1, 32, 6, 6]               0\n",
      "           Conv2d-94            [-1, 192, 6, 6]           6,144\n",
      "      BatchNorm2d-95            [-1, 192, 6, 6]             384\n",
      "            ReLU6-96            [-1, 192, 6, 6]               0\n",
      "           Conv2d-97            [-1, 192, 6, 6]           1,728\n",
      "      BatchNorm2d-98            [-1, 192, 6, 6]             384\n",
      "            ReLU6-99            [-1, 192, 6, 6]               0\n",
      "          Conv2d-100             [-1, 48, 6, 6]           9,216\n",
      "     BatchNorm2d-101             [-1, 48, 6, 6]              96\n",
      "      Bottleneck-102             [-1, 48, 6, 6]               0\n",
      "          Conv2d-103            [-1, 288, 6, 6]          13,824\n",
      "     BatchNorm2d-104            [-1, 288, 6, 6]             576\n",
      "           ReLU6-105            [-1, 288, 6, 6]               0\n",
      "          Conv2d-106            [-1, 288, 6, 6]           2,592\n",
      "     BatchNorm2d-107            [-1, 288, 6, 6]             576\n",
      "           ReLU6-108            [-1, 288, 6, 6]               0\n",
      "          Conv2d-109             [-1, 48, 6, 6]          13,824\n",
      "     BatchNorm2d-110             [-1, 48, 6, 6]              96\n",
      "      Bottleneck-111             [-1, 48, 6, 6]               0\n",
      "          Conv2d-112            [-1, 288, 6, 6]          13,824\n",
      "     BatchNorm2d-113            [-1, 288, 6, 6]             576\n",
      "           ReLU6-114            [-1, 288, 6, 6]               0\n",
      "          Conv2d-115            [-1, 288, 6, 6]           2,592\n",
      "     BatchNorm2d-116            [-1, 288, 6, 6]             576\n",
      "           ReLU6-117            [-1, 288, 6, 6]               0\n",
      "          Conv2d-118             [-1, 48, 6, 6]          13,824\n",
      "     BatchNorm2d-119             [-1, 48, 6, 6]              96\n",
      "      Bottleneck-120             [-1, 48, 6, 6]               0\n",
      "          Conv2d-121            [-1, 288, 6, 6]          13,824\n",
      "     BatchNorm2d-122            [-1, 288, 6, 6]             576\n",
      "           ReLU6-123            [-1, 288, 6, 6]               0\n",
      "          Conv2d-124            [-1, 288, 3, 3]           2,592\n",
      "     BatchNorm2d-125            [-1, 288, 3, 3]             576\n",
      "           ReLU6-126            [-1, 288, 3, 3]               0\n",
      "          Conv2d-127             [-1, 80, 3, 3]          23,040\n",
      "     BatchNorm2d-128             [-1, 80, 3, 3]             160\n",
      "      Bottleneck-129             [-1, 80, 3, 3]               0\n",
      "          Conv2d-130            [-1, 480, 3, 3]          38,400\n",
      "     BatchNorm2d-131            [-1, 480, 3, 3]             960\n",
      "           ReLU6-132            [-1, 480, 3, 3]               0\n",
      "          Conv2d-133            [-1, 480, 3, 3]           4,320\n",
      "     BatchNorm2d-134            [-1, 480, 3, 3]             960\n",
      "           ReLU6-135            [-1, 480, 3, 3]               0\n",
      "          Conv2d-136             [-1, 80, 3, 3]          38,400\n",
      "     BatchNorm2d-137             [-1, 80, 3, 3]             160\n",
      "      Bottleneck-138             [-1, 80, 3, 3]               0\n",
      "          Conv2d-139            [-1, 480, 3, 3]          38,400\n",
      "     BatchNorm2d-140            [-1, 480, 3, 3]             960\n",
      "           ReLU6-141            [-1, 480, 3, 3]               0\n",
      "          Conv2d-142            [-1, 480, 3, 3]           4,320\n",
      "     BatchNorm2d-143            [-1, 480, 3, 3]             960\n",
      "           ReLU6-144            [-1, 480, 3, 3]               0\n",
      "          Conv2d-145             [-1, 80, 3, 3]          38,400\n",
      "     BatchNorm2d-146             [-1, 80, 3, 3]             160\n",
      "      Bottleneck-147             [-1, 80, 3, 3]               0\n",
      "          Conv2d-148            [-1, 480, 3, 3]          38,400\n",
      "     BatchNorm2d-149            [-1, 480, 3, 3]             960\n",
      "           ReLU6-150            [-1, 480, 3, 3]               0\n",
      "          Conv2d-151            [-1, 480, 3, 3]           4,320\n",
      "     BatchNorm2d-152            [-1, 480, 3, 3]             960\n",
      "           ReLU6-153            [-1, 480, 3, 3]               0\n",
      "          Conv2d-154            [-1, 160, 3, 3]          76,800\n",
      "     BatchNorm2d-155            [-1, 160, 3, 3]             320\n",
      "      Bottleneck-156            [-1, 160, 3, 3]               0\n",
      "          Conv2d-157            [-1, 640, 3, 3]         102,400\n",
      "     BatchNorm2d-158            [-1, 640, 3, 3]           1,280\n",
      "           ReLU6-159            [-1, 640, 3, 3]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 640, 1, 1]               0\n",
      "          Conv2d-161           [-1, 1000, 1, 1]         641,000\n",
      "================================================================\n",
      "Total params: 1,221,672\n",
      "Trainable params: 1,221,672\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 14.89\n",
      "Params size (MB): 4.66\n",
      "Estimated Total Size (MB): 19.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def MobileNetV2():\n",
    "    bottleneck_settings = {\n",
    "                'c': [16, 24, 32, 64, 96, 160, 320],\n",
    "                't': [1, 6, 6, 6, 6, 6, 6],\n",
    "                's': [1, 2, 2, 2, 1, 2, 1],\n",
    "                'n': [1, 2, 3, 4, 3, 3, 1]\n",
    "            }\n",
    "    \n",
    "    return MobileNet(block_type=Bottleneck,\n",
    "                     bottleneck_settings=bottleneck_settings,\n",
    "                     width_multiplier=.5,\n",
    "                     num_classes=1000)\n",
    "\n",
    "model = MobileNetV2()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "summary(model, (3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8apAJeqo0t3C"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKDLjL0T2cWb"
   },
   "source": [
    "## c) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iIkCsxZ2dSd"
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    EPOCHS = 15\n",
    "    nb_examples = 45000\n",
    "    nb_val_examples = 5000\n",
    "    train_costs, val_costs = [], []\n",
    "    \n",
    "    #Training phase.\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        \n",
    "        model.train().cuda()\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Zero the parameter gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass.\n",
    "            prediction = model(inputs)\n",
    "            \n",
    "            # Compute the loss.\n",
    "            loss = criterion(prediction, labels)\n",
    "          \n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimize.\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute training accuracy.\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            correct_train += (predicted == labels).float().sum().item()\n",
    "            \n",
    "            # Compute batch loss.\n",
    "            train_loss += (loss.data.item() * inputs.shape[0])\n",
    "\n",
    "\n",
    "        train_loss /= nb_examples\n",
    "        train_costs.append(train_loss)\n",
    "        train_acc =  correct_train / nb_examples\n",
    "\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "          \n",
    "        model.eval().cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass.\n",
    "                prediction = model(inputs)\n",
    "\n",
    "                # Compute the loss.\n",
    "                loss = criterion(prediction, labels)\n",
    "\n",
    "                # Compute training accuracy.\n",
    "                _, predicted = torch.max(prediction.data, 1)\n",
    "                correct_val += (predicted == labels).float().sum().item()\n",
    "\n",
    "            # Compute batch loss.\n",
    "            val_loss += (loss.data.item() * inputs.shape[0])\n",
    "\n",
    "            val_loss /= nb_val_examples\n",
    "            val_costs.append(val_loss)\n",
    "            val_acc =  correct_val / nb_val_examples\n",
    "        \n",
    "        info = \"[Epoch {}/{}]: train-loss = {:0.6f} | train-acc = {:0.3f} | val-loss = {:0.6f} | val-acc = {:0.3f}\"\n",
    "        print(info.format(epoch+1, EPOCHS, train_loss, train_acc, val_loss, val_acc))\n",
    "        torch.save(model.state_dict(), 'save_weights/checkpoint_gpu_{}'.format(epoch + 1)) \n",
    "                                                                \n",
    "    torch.save(model.state_dict(), 'save_weights/mobilenet-v2_weights_gpu')  \n",
    "        \n",
    "    return train_costs, val_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "X6l-pla251pC",
    "outputId": "aeaa94f4-d239-4d43-eb79-bd240941cd81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 19 08:02:07 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P0    34W / 250W |  16139MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "BjElAOao2rnv",
    "outputId": "59ab914a-0822-4947-cae6-7e873d9d2669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/15]: train-loss = 1.641850 | train-acc = 0.397 | val-loss = 0.035919 | val-acc = 0.528\n",
      "[Epoch 2/15]: train-loss = 1.192912 | train-acc = 0.569 | val-loss = 0.035297 | val-acc = 0.612\n",
      "[Epoch 3/15]: train-loss = 0.997534 | train-acc = 0.642 | val-loss = 0.033239 | val-acc = 0.654\n",
      "[Epoch 4/15]: train-loss = 0.868714 | train-acc = 0.692 | val-loss = 0.028015 | val-acc = 0.696\n",
      "[Epoch 5/15]: train-loss = 0.760505 | train-acc = 0.733 | val-loss = 0.021540 | val-acc = 0.721\n",
      "[Epoch 6/15]: train-loss = 0.681773 | train-acc = 0.763 | val-loss = 0.013692 | val-acc = 0.752\n",
      "[Epoch 7/15]: train-loss = 0.617383 | train-acc = 0.786 | val-loss = 0.023159 | val-acc = 0.749\n",
      "[Epoch 8/15]: train-loss = 0.568520 | train-acc = 0.802 | val-loss = 0.017240 | val-acc = 0.749\n",
      "[Epoch 9/15]: train-loss = 0.527746 | train-acc = 0.816 | val-loss = 0.017224 | val-acc = 0.763\n",
      "[Epoch 10/15]: train-loss = 0.491380 | train-acc = 0.828 | val-loss = 0.019151 | val-acc = 0.778\n",
      "[Epoch 11/15]: train-loss = 0.469383 | train-acc = 0.837 | val-loss = 0.019102 | val-acc = 0.785\n",
      "[Epoch 12/15]: train-loss = 0.432044 | train-acc = 0.849 | val-loss = 0.022228 | val-acc = 0.785\n",
      "[Epoch 13/15]: train-loss = 0.404078 | train-acc = 0.860 | val-loss = 0.015088 | val-acc = 0.790\n",
      "[Epoch 14/15]: train-loss = 0.385579 | train-acc = 0.865 | val-loss = 0.016302 | val-acc = 0.793\n",
      "[Epoch 15/15]: train-loss = 0.363964 | train-acc = 0.872 | val-loss = 0.017327 | val-acc = 0.794\n"
     ]
    }
   ],
   "source": [
    "train_costs, val_costs = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q7clDMYz2uHw",
    "outputId": "7b110e26-9e8a-4ff6-b82f-d1f0d474a93c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Restore the model.\n",
    "model = MobileNetV2()\n",
    "model.load_state_dict(torch.load('save_weights/mobilenet-v2_weights_gpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cQBoAs052wTG",
    "outputId": "356534fe-af6c-49e4-a5dd-79c76849b82a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8007\n"
     ]
    }
   ],
   "source": [
    "nb_test_examples = 10000\n",
    "correct = 0 \n",
    "\n",
    "model.eval().cuda()\n",
    "\n",
    "with  torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Make predictions.\n",
    "        prediction = model(inputs)\n",
    "\n",
    "        # Retrieve predictions indexes.\n",
    "        _, predicted_class = torch.max(prediction.data, 1)\n",
    "\n",
    "        # Compute number of correct predictions.\n",
    "        correct += (predicted_class == labels).float().sum().item()\n",
    "\n",
    "test_accuracy = correct / nb_test_examples\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mobilenet_v2_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ea8e31611fc499995dedc0b3405f9be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "456e14f6725f42d183ffd82472813baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_956d35386dc2413eaf35645f82c1b5bd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ea8e31611fc499995dedc0b3405f9be",
      "value": 1
     }
    },
    "60689c3a4e9e450a9403d7aa47a759f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_456e14f6725f42d183ffd82472813baf",
       "IPY_MODEL_f00311b48f76429eb02379de9c257286"
      ],
      "layout": "IPY_MODEL_a8f5782fae1e4838877befe6c57350f5"
     }
    },
    "956d35386dc2413eaf35645f82c1b5bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8f5782fae1e4838877befe6c57350f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c571fbb91dc84540903ab412cd8279bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f00311b48f76429eb02379de9c257286": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f823f8422b7a490c883d00f0258fa11d",
      "placeholder": "",
      "style": "IPY_MODEL_c571fbb91dc84540903ab412cd8279bb",
      "value": " 170500096/? [00:22&lt;00:00, 16604436.16it/s]"
     }
    },
    "f823f8422b7a490c883d00f0258fa11d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
